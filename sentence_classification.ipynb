{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo\n",
    "from nemo.utils.lr_policies import get_lr_policy\n",
    "import nemo_nlp\n",
    "from nemo_nlp.utils.callbacks.sentence_classification import \\\n",
    "    eval_iter_callback, eval_epochs_done_callback\n",
    "import preproc_data_layer\n",
    "from pytorch_transformers import BertTokenizer\n",
    "import torch.nn.functional as f\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = -1\n",
    "\n",
    "import json\n",
    "\n",
    "from preproc_data_layer import BertSentenceClassificationDataset, PreprocBertSentenceClassificationDataLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Explore\n",
    "\n",
    "The SST-2 dataset https://nlp.stanford.edu/sentiment/index.html is a standard benchmark for sentence classification and is part of the GLUE Benchmark: https://gluebenchmark.com/tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/SST-2'\n",
    "df = pd.read_csv(data_dir + '/train.tsv', sep='\\t')\n",
    "test_df = pd.read_csv(data_dir + '/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes with a train file (labeled) and a test file (not labeled).  We will use part of the train file for model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train to train and val and save to disk\n",
    "np.random.seed(123)\n",
    "train_mask = np.random.rand((len(df))) < .8\n",
    "train_df = df[train_mask]\n",
    "val_df = df[~train_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to take advantage of NeMo's pre-built sentence classification data layer, the data must formatted as \"sentence\\tlabel\" (sentence tab label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will add a label column with all 0's (but they will not be used for anything).\n",
    "test_df['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[['sentence', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new train, val, and test to disk\n",
    "train_df.to_csv(data_dir + '/my_train.tsv', sep='\\t', index=False)\n",
    "val_df.to_csv(data_dir + '/my_val.tsv', sep='\\t', index=False)\n",
    "test_df.to_csv(data_dir + '/my_test.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In order to use BERT or any other Deep NLP based model, we must first tokenize the data. Our tokenizer will map each word to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each pretrained BERT model comes with a Tokenizer\n",
    "pretrained_bert_model = 'bert-base-uncased'\n",
    "#pretrained_bert_model = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 64 # we will pad 0's to shorter sentences and truncate longer\n",
    "sample_dataset = BertSentenceClassificationDataset(\n",
    "data_dir + '/my_train.tsv',\n",
    "max_seq_length,\n",
    "tokenizer,\n",
    "num_samples=100,\n",
    "shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokenization(tokenizer, tokens):\n",
    "    pad_counter = 0\n",
    "    for token in tokens:\n",
    "        if token == 0:\n",
    "            pad_counter += 1\n",
    "        if pad_counter > 4:\n",
    "            break\n",
    "        vocab_str = list(tokenizer.vocab.keys())[list(tokenizer.vocab.values()).index(token)]\n",
    "        print(f'{vocab_str:15s} | {token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = np.random.randint(0, len(sample_dataset))\n",
    "tokens = sample_dataset[sample_idx][0]\n",
    "print_tokenization(tokenizer, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can gain a lot of efficiency by saving the tokenized data to disk. For future model runs we then don't need to tokenize every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preproc_data.py \\\n",
    "--input_file $f'{data_dir}/my_train.tsv' \\\n",
    "--output_dir $f'{data_dir}/preproc' \\\n",
    "--dataset_name 'train-sst-2' \\\n",
    "--max_seq_length $max_seq_length \\\n",
    "--pretrained_bert_model $pretrained_bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preproc_data.py \\\n",
    "--input_file $f'{data_dir}/my_test.tsv' \\\n",
    "--output_dir $f'{data_dir}/preproc' \\\n",
    "--dataset_name \"test-sst-2\" \\\n",
    "--max_seq_length $max_seq_length \\\n",
    "--pretrained_bert_model $pretrained_bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preproc_data.py \\\n",
    "--input_file $f'{data_dir}/my_val.tsv' \\\n",
    "--output_dir $f'{data_dir}/preproc' \\\n",
    "--dataset_name \"val-sst-2\" \\\n",
    "--max_seq_length $max_seq_length \\\n",
    "--pretrained_bert_model $pretrained_bert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Modules\n",
    "\n",
    "In NeMo, everything is a Neural Module. Neural modules abstract data and neural network architectures. Where a deep learning framework like PyTorch or Tensorflow is used to combine neural network layers to create a neural network, NeMo is used to combine data and neural networks to create AI applications.\n",
    "\n",
    "The Neural Module Factory will then manage the neural modules, taking care to flow data through the neural modules, and is also responsible for training (including mixed precision and distributed), logging, and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the neural module factory\n",
    "log_dir = 'logs_' + pretrained_bert_model\n",
    "checkpoint_dir = 'checkpoints_' + pretrained_bert_model\n",
    "tensorboard_dir = 'tensorboard_' + pretrained_bert_model\n",
    "nf = nemo.core.NeuralModuleFactory(log_dir=log_dir,\n",
    "                                   checkpoint_dir=checkpoint_dir,\n",
    "                                   tensorboard_dir=tensorboard_dir,\n",
    "                                   create_tb_writer=True,\n",
    "                                   add_time_to_log_dir=False,\n",
    "                                   optimization_level='O1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained models will be automatically downloaded and cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained BERT\n",
    "bert = nemo_nlp.BERT(pretrained_model_name=pretrained_bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same bert model config for later use\n",
    "bert_config_dict = bert.config.to_dict()\n",
    "\n",
    "bert_model_config_path = pretrained_bert_model + '_config.json'\n",
    "with open(bert_model_config_path, 'w+') as json_file:\n",
    "    json.dump(bert_config_dict, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that the BERT models we are working with are massive. This gives our models a large capacity for learning that is needed to understand the nuance and complexity of natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{pretrained_bert_model} has {bert.num_weights} weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define and instantiate the feed forward network that takes as input our BERT embeddings. This network will be used to output the sentence classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp classifier\n",
    "bert_hidden_size = bert.local_parameters['hidden_size']\n",
    "\n",
    "mlp = nemo_nlp.SequenceClassifier(hidden_size=bert_hidden_size, \n",
    "                                  num_classes=2,\n",
    "                                  num_layers=2,\n",
    "                                  log_softmax=False,\n",
    "                                  dropout=0.1)\n",
    "\n",
    "loss = nemo.backends.pytorch.common.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared to the BERT model, the MLP is tiny.\n",
    "print(f'MLP has {mlp.num_weights} weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Pipelines are used to define how data will flow the different neural networks. In this case, our data will flow through the BERT network and then the MLP network.\n",
    "\n",
    "We also have different pipelines for training, validation, and inference data.  \n",
    "\n",
    "For training data, we want it to be used for optimization so it must be shuffled and we also need to compute the loss.\n",
    "\n",
    "For validation data, we won't use it for optimization but we want to know the loss.\n",
    "\n",
    "And for inference data, we only want the final predictions coming from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_preproc = True\n",
    "\n",
    "if pretrained_bert_model == 'bert-base-uncased':\n",
    "    batch_size = 256\n",
    "if pretrained_bert_model == 'bert-large-uncased':\n",
    "    batch_size = 64\n",
    "\n",
    "if use_preproc:\n",
    "    train_data = preproc_data_layer.PreprocBertSentenceClassificationDataLayer(\n",
    "        input_file=f'{data_dir}/preproc/train-sst-2_{pretrained_bert_model}_{max_seq_length}.hdf5',\n",
    "        shuffle=True,\n",
    "        num_samples=-1, # lower for dev, -1 for all dataset\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    val_data = preproc_data_layer.PreprocBertSentenceClassificationDataLayer(\n",
    "        input_file=f'{data_dir}/preproc/val-sst-2_{pretrained_bert_model}_{max_seq_length}.hdf5',\n",
    "        shuffle=False,\n",
    "        num_samples=-1, # lower for dev, -1 for all dataset\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "else:\n",
    "    train_data = preproc_data_layer.BertSentenceClassificationDataLayer(\n",
    "        input_file=data_dir + '/my_train.tsv',\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length,\n",
    "        shuffle=True,\n",
    "        num_samples=-1, # lower for dev, -1 for all dataset\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    val_data = preproc_data_layer.BertSentenceClassificationDataLayer(\n",
    "        input_file=data_dir + '/my_val.tsv',\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length,\n",
    "        shuffle=False,\n",
    "        num_samples=-1, # lower for dev, -1 for all dataset\n",
    "        batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_token_types, train_attn_mask, train_labels = train_data()\n",
    "val_input, val_token_types, val_attn_mask, val_labels = val_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = bert(input_ids=train_input,\n",
    "                        token_type_ids=train_token_types,\n",
    "                        attention_mask=train_attn_mask)\n",
    "val_embeddings = bert(input_ids=val_input,\n",
    "                        token_type_ids=val_token_types,\n",
    "                        attention_mask=val_attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect BERT Embeddings\n",
    "\n",
    "If we want to inspect the data as it flows through our neural factory we can use the .infer method.  This method will give us the tensors without performing any optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_input_tensors = nf.infer(tensors=[val_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_input_tensors[0][0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_input_tensors[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "val_embeddings_tensors = nf.infer(tensors=[val_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each word is embedded into bert_hidden_size space\n",
    "# max_seq_len words are embedded\n",
    "print(val_embeddings_tensors[0][0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings_tensors[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings_tensors[0][0][0][:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_embeddings_tensors[0][0][1][:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding and Visualizing BERT Embeddings\n",
    "\n",
    "We are going to look at the BERT embeddings for the words (1-word sentences) in \"data/SST-2/positive_negative.tsv\". Since the BERT embeddings are 768 dimensional for BERT base and 1024 dimensional for BERT large, we'll first apply TSNE and reduce the embeddings to two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_df = pd.read_csv('data/positive_negative.tsv', delimiter='\\t')\n",
    "print(spectrum_df.sentence.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive negative spectrum\n",
    "spectrum_data = nemo_nlp.BertSentenceClassificationDataLayer(\n",
    "    input_file='data/positive_negative.tsv',\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    shuffle=False,\n",
    "    num_samples=-1, # lower for dev, -1 for all dataset\n",
    "    batch_size=batch_size,\n",
    "    dataset_type=preproc_data_layer.BertSentenceClassificationDataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_input, spectrum_token_types, spectrum_attn_mask, spectrum_labels = spectrum_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_embeddings = bert(input_ids=spectrum_input,\n",
    "                        token_type_ids=spectrum_token_types,\n",
    "                        attention_mask=spectrum_attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_embeddings_tensors = nf.infer(tensors=[spectrum_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_embeddings_tensors[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_embeddings_tensors[0][0][:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,100))\n",
    "plt.imshow(spectrum_embeddings_tensors[0][0][:,0,:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spectrum_activations = spectrum_embeddings_tensors[0][0][:,0,:].numpy()\n",
    "tsne_spectrum = TSNE(n_components=2, perplexity=10, verbose=1, learning_rate=2,\n",
    "                     random_state=123).fit_transform(spectrum_activations)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(tsne_spectrum[0:11, 0], tsne_spectrum[0:11, 1], 'rx')\n",
    "plt.plot(tsne_spectrum[11:, 0], tsne_spectrum[11:, 1], 'bo')\n",
    "for (x,y, label) in zip(tsne_spectrum[0:, 0], tsne_spectrum[0:, 1], spectrum_df.sentence.values.tolist() ):\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline \n",
    "\n",
    "In order to optimize our network, we need to pass the embeddings through the MLP network and then compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logits = mlp(hidden_states=train_embeddings)\n",
    "val_logits = mlp(hidden_states=val_embeddings)\n",
    "\n",
    "train_loss = loss(logits=train_logits, labels=train_labels)\n",
    "val_loss = loss(logits=val_logits, labels=val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Callbacks are used to record and log metrics and save checkpoints for the training and evaluation. We use callbacks to print to screen and also to tensorboard.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "num_gpus = 1\n",
    "\n",
    "train_data_size = len(train_data)\n",
    "\n",
    "steps_per_epoch = math.ceil(train_data_size / (batch_size * num_gpus))\n",
    "\n",
    "train_callback = nemo.core.SimpleLossLoggerCallback(\n",
    "    tensors=[train_loss, train_logits],\n",
    "    print_func=lambda x: nf.logger.info(f'Train loss: {str(np.round(x[0].item(), 3))}'),\n",
    "    tb_writer=nf.tb_writer,\n",
    "    get_tb_values=lambda x: [[\"train_loss\", x[0]]],\n",
    "    step_freq=1)\n",
    "\n",
    "eval_callback = nemo.core.EvaluatorCallback(\n",
    "    eval_tensors=[val_logits, val_labels],\n",
    "    user_iter_callback=lambda x, y: eval_iter_callback(\n",
    "        x, y, val_data),\n",
    "    user_epochs_done_callback=lambda x: eval_epochs_done_callback(\n",
    "        x, f'{nf.work_dir}/graphs'),\n",
    "    tb_writer=nf.tb_writer,\n",
    "    eval_epoch=1,\n",
    "    eval_step=steps_per_epoch)\n",
    "\n",
    "# Create callback to save checkpoints\n",
    "ckpt_callback = nemo.core.CheckpointCallback(\n",
    "    folder=nf.checkpoint_dir,\n",
    "    epoch_freq=1,\n",
    "    step_freq=-1,\n",
    "    checkpoints_to_keep=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_policy_fn = get_lr_policy('WarmupAnnealing',\n",
    "                             total_steps=num_epochs * steps_per_epoch,\n",
    "                             warmup_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nf.train(tensors_to_optimize=[train_loss],\n",
    "         callbacks=[train_callback, eval_callback, ckpt_callback],\n",
    "         lr_policy=lr_policy_fn,\n",
    "         optimizer='adam',\n",
    "         optimization_params={'num_epochs': num_epochs, 'lr': 5e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensorboard_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # command for distributed training\n",
    "# time python -m torch.distributed.launch --nproc_per_node=2 sentence_classification.py \\\n",
    "# --train_file $f'{data_dir}/preproc/train-sst-2_{pretrained_bert_model}_{max_seq_length}.hdf5' \\\n",
    "# --eval_file $f'{data_dir}/preproc/train-sst-2_{pretrained_bert_model}_{max_seq_length}.hdf5' \\\n",
    "# --num_gpus 2 \\\n",
    "# --batch_size $batch_size \\\n",
    "# --amp_opt_level O1 \\\n",
    "# --work_dir $f'distributed_logs_{pretrained_bert_model}' \\\n",
    "# --mode train \\\n",
    "# --num_classes 2 \\\n",
    "# --num_samples -1 \\\n",
    "# --num_epochs 1 \\\n",
    "# --preproc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
